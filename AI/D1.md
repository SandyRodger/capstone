### intro to AI applications

<img width="1128" height="634" alt="Screenshot 2025-09-15 at 16 08 00" src="https://github.com/user-attachments/assets/b1a59c26-544e-46de-89ec-12fb6dfa45d9" />

- yes it's just an API call.
- BUT behind this there are a lot of software engineering challenges, which we wil look at this week.
- at the end of the week it'll be more like this:
<img width="1440" height="900" alt="Screenshot 2025-09-15 at 16 08 41" src="https://github.com/user-attachments/assets/7ac54766-1b83-4cbf-8764-490de6e1bc69" />

<img width="1440" height="900" alt="Screenshot 2025-09-15 at 16 09 34" src="https://github.com/user-attachments/assets/6adead75-f3b9-45a1-863d-e0ac2878fed3" />

- We're using OpenAI, no frameworks. Most big Ai APIs are similar.
- Homework is for learning, lecture is for discussing. Conversations will be guided by wherever the questions go.
- So come to class with questions.
- Office hours at 1pm Pacific (9pm English) Open to groups, pop in.
- Wednesday -> Explaining LSbot Q&A 9pm English (same zoom) 

### Overview

<img width="1440" height="900" alt="Screenshot 2025-09-15 at 16 15 32" src="https://github.com/user-attachments/assets/00dd27ae-810a-4aa8-9700-5e36d7c028f1" />

Auto-regressive: what comes next
masked language models: what comes after?
- supervised v non-supervised: type of data labelled by a human or automated = TRAINING
- Labelled data is gold-standard, but it costs a lot! So you might label 10% to jump start and the rest will be automated.
- The purpose of training is to generate/tune the parameters of your models.
- The people designing the model choose how many paramers they want.
- Supervised/unsupervised is about cost-effectivness.
- hyper-parameters: of the most likely tokens, how are we going to choose.
  - temperature: how likely it is to choose the most likely.
- prompt template-> a higher level system prompt
- Hallucinations? A model is only as good as its training data. It's always going to give an output, but it will be dependent on its prompt and its data and the longer it gets the more likely.
- Context-length: how much text you can put in the window. It was a bigger problem a few years ago. Now we can put in much more (like a million tokens)

### size
### Training
- re-enforcement learning with human-training:
  - asking humans to judge output
- pre-training.
- build an intuition for these terms, but you don't need to memorise them now.
- 'frontier models'

### Tokens
- understand them in terms of numerical operations within a model. They are transformed into an array of numbers

- top p “Top-p sampling picks the next word from the smallest set of likely options that together cover p% of probability — balancing randomness with coherence.”
- 
